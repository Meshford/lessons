{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† –£—Ä–æ–∫ 36: –ó–∞–¥–∞—á–∏ NLP ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, NER\n",
        "**–¶–µ–ª—å —É—Ä–æ–∫–∞:** –ü–æ–Ω—è—Ç—å —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏ NLP ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER), –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (BERT, GPT, spaCy) –¥–ª—è –∏—Ö —Ä–µ—à–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤.\n",
        "\n",
        "## üìå –ß—Ç–æ —Ç–∞–∫–æ–µ NLP?\n",
        "- **NLP (Natural Language Processing)** ‚Äî —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å, –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç [[4]](https://example.com).\n",
        "- **–ó–∞—á–µ–º?** –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ —á–∞—Ç-–±–æ—Ç–∞—Ö, –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏.\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç ‚Äî —ç—Ç–æ —è–∑—ã–∫, —Ç–æ NLP ‚Äî —ç—Ç–æ –∫–∞–∫ —É—á–∏—Ç—å —è–∑—ã–∫, —Ä–∞–∑–±–∏–≤–∞—è –µ–≥–æ –Ω–∞ —Å–ª–æ–≤–∞, —Ñ—Ä–∞–∑—ã –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è [[8]](https://example.com).\n",
        "- **–ö–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏ NLP:**\n",
        "  - **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:** –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–∫—Å–∏—á–Ω—ã–π/–Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã–π).\n",
        "  - **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:** –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤, –æ—Ç–≤–µ—Ç–æ–≤, —Å—Ç–∞—Ç–µ–π.\n",
        "  - **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER):** –ü–æ–∏—Å–∫ –∏–º–µ–Ω, –¥–∞—Ç, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –≤ —Ç–µ–∫—Å—Ç–µ [[2]](https://example.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß± –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –æ—Ç –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–æ BERT\n",
        "- **–ß—Ç–æ —ç—Ç–æ?** –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤).\n",
        "- **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
        "  - **–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:** –õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å, –∏—â–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å.\n",
        "  - **BERT:** –î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –º–æ–¥–µ–ª—å, —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ –∏ –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞.\n",
        "- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ BERT:**\n",
        "  - –õ—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"—è–±–ª–æ–∫–æ\" –∫–∞–∫ –∫–æ–º–ø–∞–Ω–∏—è –∏ —Ñ—Ä—É–∫—Ç).\n",
        "  - –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö.\n",
        "- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n",
        "  - –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, —á–µ–º –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏.\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "  inputs = tokenizer(\"–≠—Ç–æ—Ç —Ñ–∏–ª—å–º –ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π!\", return_tensors='tf')\n",
        "  outputs = model(inputs)\n",
        "  predicted_class = tf.argmax(outputs.logits, axis=1).numpy()\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ‚Äî –∫–∞–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–∏—Å–µ–º –ø–æ —Ç–µ–º–∞–º (—Å–ø–∞–º/–Ω–µ —Å–ø–∞–º)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìê –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: GPT –∏ –µ–≥–æ –∞–Ω–∞–ª–æ–≥–∏\n",
        "- **–ß—Ç–æ —ç—Ç–æ?** –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ prompt.\n",
        "- **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
        "  - **GPT:** –û–±—É—á–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—à–ª—ã—Ö.\n",
        "  - **Causal Language Model:** –£—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–µ (–Ω–µ—Ç –±—É–¥—É—â–∏—Ö —Å–ª–æ–≤).\n",
        "- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ GPT:**\n",
        "  - –ú–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ª—é–±–æ–π –¥–ª–∏–Ω—ã.\n",
        "  - –•–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –ª–æ–≥–∏–∫–æ–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏.\n",
        "- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n",
        "  - –ú–æ–∂–µ—Ç ¬´–≤—Ä–∞—Ç—å¬ª, –µ—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—Ç –æ—Ç–≤–µ—Ç–∞ [[7]](https://example.com).\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "  model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "  inputs = tokenizer(\"–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\", return_tensors='tf')\n",
        "  outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "  print(tokenizer.decode(outputs[0]))\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ‚Äî –∫–∞–∫ –ø–∏—Å–∞—Ç–µ–ª—å, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–º [[1]](https://example.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Named Entity Recognition (NER): –æ—Ç BERT –¥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
        "- **–ß—Ç–æ —ç—Ç–æ?** –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–∏–º–µ–Ω–∞, –¥–∞—Ç—ã, –º–µ—Å—Ç–∞) –∏–∑ —Ç–µ–∫—Å—Ç–∞ [[5]](https://example.com).\n",
        "- **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
        "  - **spaCy:** –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∏—â–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤–∞—Ä–µ–π –∏ –ø—Ä–∞–≤–∏–ª.\n",
        "  - **BERT:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π.\n",
        "  - **–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏:** –°–æ–∑–¥–∞—é—Ç –Ω–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞.\n",
        "- **–ü—Ä–∏–º–µ—Ä—ã —Å—É—â–Ω–æ—Å—Ç–µ–π:**\n",
        "  - **PER:** –ü–µ—Ä—Å–æ–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤\")\n",
        "  - **LOC:** –ú–µ—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ú–æ—Å–∫–≤–∞\")\n",
        "  - **ORG:** –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"Google\")\n",
        "- **–ü—Ä–∏–º–µ—Ä —Å spaCy:**\n",
        "  ```python\n",
        "  import spacy\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  doc = nlp(\"Apple –æ—Å–Ω–æ–≤–∞–Ω–∞ –≤ –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏ –°—Ç–∏–≤–æ–º –î–∂–æ–±—Å–æ–º.\")\n",
        "  for ent in doc.ents:\n",
        "      print(ent.text, ent.label_)\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** NER ‚Äî –∫–∞–∫ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ –∏–º–µ–Ω –∏ –º–µ—Å—Ç –≤ —É—á–µ–±–Ω–∏–∫–µ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ —ç–∫–∑–∞–º–µ–Ω—É [[1]](https://example.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ –ü—Ä–∞–∫—Ç–∏–∫–∞: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å BERT\n",
        "### –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=['sci.space', 'rec.sport.baseball'])\n",
        "X_train = newsgroups.data\n",
        "y_train = newsgroups.target\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=['sci.space', 'rec.sport.baseball'])\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –®–∞–≥ 2: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length=512)\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "import tensorflow as tf\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_dataset.shuffle(100).batch(16), epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞\n",
        "loss, accuracy = model.evaluate(test_dataset.batch(16))\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà –ü—Ä–∞–∫—Ç–∏–∫–∞: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å GPT\n",
        "### –®–∞–≥ 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "inputs = tokenizer(\"–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\", return_tensors='tf')\n",
        "outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ API (GPT-3/ChatGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"your-api-key\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt=\"–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏\",\n",
        "  max_tokens=150\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä –ü—Ä–∞–∫—Ç–∏–∫–∞: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)\n",
        "### –®–∞–≥ 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"Apple –æ—Å–Ω–æ–≤–∞–Ω–∞ –≤ –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏ –°—Ç–∏–≤–æ–º –î–∂–æ–±—Å–æ–º. Google ‚Äî –∫—Ä—É–ø–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è –∏–∑ –°–®–ê.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# –í—ã–≤–æ–¥ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "displacy.render(doc, style='ent', jupyter=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìâ –ß—Ç–æ —Ç–∞–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –∫–∞–∫ –µ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å?\n",
        "- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (Overfitting):** –ú–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–æ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö [[7]](https://example.com).\n",
        "- **–ü—Ä–∏—á–∏–Ω—ã:**\n",
        "  - –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
        "  - –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö.\n",
        "- **–ö–∞–∫ –±–æ—Ä–æ—Ç—å—Å—è?**\n",
        "  - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (Dropout, L2).\n",
        "  - –£–≤–µ–ª–∏—á—å—Ç–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è).\n",
        "  - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.\n",
        "- **–ü—Ä–∏–º–µ—Ä —Å Dropout:**\n",
        "  ```python\n",
        "  from transformers import TFBertForSequenceClassification\n",
        "  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "  model.add_layer(Dropout(0.5))\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Äî –∫–∞–∫ –∑—É–±—Ä–µ–∂–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤, –∞ –Ω–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–º—ã."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã?\n",
        "- **GridSearchCV:** –ü–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π.\n",
        "- **RandomizedSearchCV:** –°–ª—É—á–∞–π–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä (–±—ã—Å—Ç—Ä–µ–µ, —á–µ–º GridSearch).\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  param_grid = {'learning_rate': [1e-5, 3e-5], 'batch_size': [8, 16, 32]}\n",
        "  grid = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
        "  grid.fit(X_train, y_train)\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Äî –∫–∞–∫ –ø–æ–∏—Å–∫ –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ —Ä–µ—Ü–µ–ø—Ç–∞ —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ\n",
        "**–ó–∞–¥–∞—á–∞ 1:** –ò–∑–º–µ–Ω–∏—Ç–µ prompt –¥–ª—è GPT-2:\n",
        "- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ö–∞–∫ —Å—Ç–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º\", \"–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ –º–µ–¥–∏—Ü–∏–Ω–µ\").\n",
        "- –ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏?\n",
        "- –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª.\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞ 2:** –û–±—É—á–∏—Ç–µ BERT –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ IMDB (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∑—ã–≤–æ–≤):\n",
        "- –°–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é `fetch_openml`.\n",
        "- –û—Ü–µ–Ω–∏—Ç–µ —Ç–æ—á–Ω–æ—Å—Ç—å.\n",
        "- –ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç—á–µ—Ç (200‚Äì300 —Å–ª–æ–≤), –≥–¥–µ:\n",
        "  - –û–ø–∏—à–∏—Ç–µ, –∫–∞–∫ –≤—ã –æ–±—É—á–∞–ª–∏ –º–æ–¥–µ–ª—å.\n",
        "  - –°—Ä–∞–≤–Ω–∏—Ç–µ —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π.\n",
        "  - –û–±—ä—è—Å–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É BERT —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö.\n",
        "  - –ü—Ä–∏–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ NER –ø–æ–ª–µ–∑–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Å–Ω–∏–º–∫–∏, –∞–Ω–∞–ª–∏–∑ —Å–æ—Ü—Å–µ—Ç–µ–π [[5]](https://example.com)).\n",
        "- **–ü–æ–¥—Å–∫–∞–∑–∫–∞:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `sns.heatmap` –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ prompt\n",
        "prompts = [\n",
        "    \"–ö–∞–∫ —Å—Ç–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º?\",\n",
        "    \"–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ –º–µ–¥–∏—Ü–∏–Ω–µ\",\n",
        "    \"–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤\"\n",
        "]\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors='tf')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
        "    print(f'\\nPrompt: {prompt}')\n",
        "    print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ BERT –Ω–∞ IMDB\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "X = X / 255.0\n",
        "y = np.array([int(label) % 2 for label in y])\n",
        "X_train, X_test = X[:60000], X[60000:]\n",
        "y_train, y_test = y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ BERT\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length=512)\n",
        "train_encodings = tokenizer(X_train.astype(str).tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(X_test.astype(str).tolist(), truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_encodings, y_train, epochs=3, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞\n",
        "loss, accuracy = model.evaluate(test_encodings, y_test)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é\n",
        "- **–ó–∞–¥–∞—á–∞ 1:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ prompt –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è.\n",
        "- **–ó–∞–¥–∞—á–∞ 2:** –î–ª—è BERT –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `TFBertForSequenceClassification` –∏ `tokenizer`.\n",
        "- **–ü–æ–¥—Å–∫–∞–∑–∫–∞:** –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è, —É–º–µ–Ω—å—à–∏—Ç–µ `num_train_epochs` –∏–ª–∏ —É–≤–µ–ª–∏—á—å—Ç–µ `weight_decay`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
