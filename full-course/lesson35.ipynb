{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† –£—Ä–æ–∫ 35: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –º–æ–¥–µ–ª–∏ BERT, GPT\n",
        "**–¶–µ–ª—å —É—Ä–æ–∫–∞:** –ü–æ–Ω—è—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, BERT –∏ GPT, –∏ –Ω–∞—É—á–∏—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –∑–∞–¥–∞—á NLP. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤.\n",
        "\n",
        "## üìå –ó–∞—á–µ–º –Ω—É–∂–Ω—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã?\n",
        "- **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã** —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN/LSTM).\n",
        "- **Self-Attention:** –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –¥–∞–ª–µ–∫–æ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞.\n",
        "- **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è:** –û–±—É—á–µ–Ω–∏–µ –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —É —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π.\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä ‚Äî –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç –≤—Å—é –∫–Ω–∏–≥—É —Å—Ä–∞–∑—É –∏ –ø–æ–Ω–∏–º–∞–µ—Ç, –∫—Ç–æ –µ—Å—Ç—å –∫—Ç–æ, –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –∂–¥–∞—Ç—å –∫–æ–Ω—Ü–∞ –≥–ª–∞–≤—ã [[2]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìê –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention)\n",
        "- **–í–Ω–∏–º–∞–Ω–∏–µ (Attention):** –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∂–Ω—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞.\n",
        "- **Self-Attention:** –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ –≤—Å–µ–º–∏ –¥—Ä—É–≥–∏–º–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.\n",
        "- **Q (Query), K (Key), V (Value):**\n",
        "  - Query ‚Äî —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ –∏—â–µ—Ç —Å–≤—è–∑—å.\n",
        "  - Key ‚Äî –≤—Å–µ —Å–ª–æ–≤–∞, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ Query —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è.\n",
        "  - Value ‚Äî –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n",
        "- **–§–æ—Ä–º—É–ª–∞:**\n",
        "  ```python\n",
        "  attention_weights = softmax(Q @ K.T / sqrt(d_k))\n",
        "  output = attention_weights @ V\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** Self-Attention ‚Äî –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç \"Apple\" –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ –∞–±–∑–∞—Ü–∞ [[1]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß± –ß—Ç–æ —Ç–∞–∫–æ–µ BERT?\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers):** –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–±–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è).\n",
        "- **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
        "  - **Masked Language Model (MLM):** –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–π–Ω–æ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞.\n",
        "  - **Next Sentence Prediction (NSP):** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∏–¥—É—Ç –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥—Ä—É–≥ –∑–∞ –¥—Ä—É–≥–æ–º.\n",
        "- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "  - –ü–æ–Ω–∏–º–∞–µ—Ç –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"—è–±–ª–æ–∫–æ\" –∫–∞–∫ –∫–æ–º–ø–∞–Ω–∏—è –∏ –∫–∞–∫ —Ñ—Ä—É–∫—Ç).\n",
        "  - –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, NER.\n",
        "- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n",
        "  - –ù–µ —É–º–µ–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç.\n",
        "  - –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from transformers import BertTokenizer, TFBertModel\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "  inputs = tokenizer(\"–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!\", return_tensors='tf')\n",
        "  outputs = model(inputs)\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** BERT ‚Äî –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç –∫–Ω–∏–≥—É –∏ –ø–æ–º–Ω–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ –∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ [[1]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà –ß—Ç–æ —Ç–∞–∫–æ–µ GPT?\n",
        "- **GPT (Generative Pretrained Transformer):** –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ.\n",
        "- **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
        "  - **Decoder-only:** –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –æ–ø–∏—Ä–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–ª–æ–≤–∞.\n",
        "  - **Causal Language Model:** –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—à–ª—ã—Ö.\n",
        "- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "  - –ú–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã –ª—é–±–æ–π –¥–ª–∏–Ω—ã.\n",
        "  - –•–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –ª–æ–≥–∏–∫–æ–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏.\n",
        "- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n",
        "  - –ù–µ –≤–∏–¥–∏—Ç –±—É–¥—É—â–∏—Ö —Å–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–µ).\n",
        "  - –ú–æ–∂–µ—Ç ¬´–≤—Ä–∞—Ç—å¬ª, –µ—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—Ç –æ—Ç–≤–µ—Ç–∞ [[6]].\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "  model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "  inputs = tokenizer(\"–ü–æ—á–µ–º—É –Ω–µ–±–æ –≥–æ–ª—É–±–æ–µ?\", return_tensors='tf')\n",
        "  outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "  print(tokenizer.decode(outputs[0]))\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** GPT ‚Äî –∫–∞–∫ –ø–∏—Å–∞—Ç–µ–ª—å, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–º [[4]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ BERT –∏ GPT\n",
        "- **BERT:** –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π, –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
        "- **GPT:** –û–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π, –ª—É—á—à–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –Ω–æ –º–æ–∂–µ—Ç –æ—à–∏–±–∞—Ç—å—Å—è –≤ —Ñ–∞–∫—Ç–∞—Ö.\n",
        "- **–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:**\n",
        "  - **BERT:** –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, NER, Q&A.\n",
        "  - **GPT:** –ß–∞—Ç-–±–æ—Ç—ã, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –ø–µ—Ä–µ–≤–æ–¥.\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** BERT ‚Äî –∫–∞–∫ —Å—Ç—É–¥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ—á–∏—Ç–∞–ª —É—á–µ–±–Ω–∏–∫ –∏ –º–æ–∂–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, GPT ‚Äî –∫–∞–∫ –ø–∏—Å–∞—Ç–µ–ª—å, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–ø—ã—Ç–∞ [[5]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ –ü—Ä–∞–∫—Ç–∏–∫–∞: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ BERT –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "### –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "df = pd.DataFrame({\n",
        "    'text': [\n",
        "        \"–Ø –ª—é–±–ª—é Python!\",\n",
        "        \"Java ‚Äî –º–æ–π –ª—é–±–∏–º—ã–π —è–∑—ã–∫.\",\n",
        "        \"Python –ª—É—á—à–µ –≤—Å–µ—Ö!\",\n",
        "        \"–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è C++\",\n",
        "        \"–õ—é–±–ª—é –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\",\n",
        "        \"–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫—Ä—É—Ç–æ\",\n",
        "        \"–Ø –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –Ω–∞ Python\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 1, 1]  # 1 = –ª—é–±–∏—Ç Python, 0 = –Ω–µ—Ç\n",
        "})\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –®–∞–≥ 2: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_dataset.shuffle(100).batch(16), epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞\n",
        "loss, accuracy = model.evaluate(test_dataset.batch(16))\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà –ü—Ä–∞–∫—Ç–∏–∫–∞: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å GPT\n",
        "### –®–∞–≥ 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "inputs = tokenizer(\"–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\", return_tensors='tf')\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –®–∞–≥ 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPT-3 (—á–µ—Ä–µ–∑ API)\n",
        "```python\n",
        "# –ü—Ä–∏–º–µ—Ä —á–µ—Ä–µ–∑ OpenAI API (—Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–ª—é—á)\n",
        "import openai\n",
        "openai.api_key = \"your-api-key\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt=\"–ü–æ—á–µ–º—É –Ω–µ–±–æ –≥–æ–ª—É–±–æ–µ?\",\n",
        "  max_tokens=100\n",
        ")\n",
        "print(response.choices[0].text)\n",
        "```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** GPT-3 ‚Äî –∫–∞–∫ —É—á–∏—Ç–µ–ª—å, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ [[6]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìâ –ß—Ç–æ —Ç–∞–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –∫–∞–∫ –µ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å?\n",
        "- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (Overfitting):** –ú–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–æ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö.\n",
        "- **–ü—Ä–∏—á–∏–Ω—ã:**\n",
        "  - –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
        "  - –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö.\n",
        "- **–ö–∞–∫ –±–æ—Ä–æ—Ç—å—Å—è?**\n",
        "  - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Dropout.\n",
        "  - –î–æ–±–∞–≤—å—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é.\n",
        "  - –£–≤–µ–ª–∏—á—å—Ç–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è).\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from transformers import TrainingArguments, Trainer\n",
        "  \n",
        "  training_args = TrainingArguments(\n",
        "      output_dir='./results',\n",
        "      num_train_epochs=3,\n",
        "      per_device_train_batch_size=16,\n",
        "      per_device_eval_batch_size=16,\n",
        "      warmup_steps=500,\n",
        "      weight_decay=0.01,\n",
        "      logging_dir='./logs',\n",
        "  )\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Äî –∫–∞–∫ –∑—É–±—Ä–µ–∂–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤, –∞ –Ω–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–º—ã ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã?\n",
        "- **GridSearchCV:** –ü–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π.\n",
        "- **RandomizedSearchCV:** –°–ª—É—á–∞–π–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä (–±—ã—Å—Ç—Ä–µ–µ, —á–µ–º GridSearch).\n",
        "- **–ü—Ä–∏–º–µ—Ä:**\n",
        "  ```python\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  param_grid = {'learning_rate': [1e-5, 3e-5, 5e-5], 'batch_size': [8, 16, 32]}\n",
        "  grid = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
        "  grid.fit(X_train, y_train)\n",
        "  ```\n",
        "- **–ê–Ω–∞–ª–æ–≥–∏—è:** –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Äî –∫–∞–∫ –ø–æ–∏—Å–∫ –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ —Ä–µ—Ü–µ–ø—Ç–∞ —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏ [[7]]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ\n",
        "**–ó–∞–¥–∞—á–∞ 1:** –ò–∑–º–µ–Ω–∏—Ç–µ prompt –¥–ª—è GPT-2:\n",
        "- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ö–∞–∫ —Å—Ç–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º\", \"–ü–æ—á–µ–º—É —Å–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏—Ç\").\n",
        "- –ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏?\n",
        "- –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª.\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞ 2:** –û–±—É—á–∏—Ç–µ BERT –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ IMDB (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∑—ã–≤–æ–≤):\n",
        "- –°–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é `fetch_openml`.\n",
        "- –û—Ü–µ–Ω–∏—Ç–µ —Ç–æ—á–Ω–æ—Å—Ç—å.\n",
        "- –ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç—á–µ—Ç (200‚Äì300 —Å–ª–æ–≤), –≥–¥–µ:\n",
        "  - –û–ø–∏—à–∏—Ç–µ, –∫–∞–∫ –≤—ã –æ–±—É—á–∞–ª–∏ –º–æ–¥–µ–ª—å.\n",
        "  - –°—Ä–∞–≤–Ω–∏—Ç–µ —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π.\n",
        "  - –û–±—ä—è—Å–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É BERT —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö.\n",
        "  - –ü—Ä–∏–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø–æ–ª–µ–∑–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∞—Ç-–±–æ—Ç—ã, –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ prompt\n",
        "prompts = [\n",
        "    \"–ö–∞–∫ —Å—Ç–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º?\",\n",
        "    \"–ü–æ—á–µ–º—É —Å–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏—Ç?\",\n",
        "    \"–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?\"\n",
        "]\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors='tf')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
        "    print(f'\\nPrompt: {prompt}')\n",
        "    print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ IMDB –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "X = X / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "y = np.array([int(label) % 2 for label in y])  # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è: —á–µ—Ç–Ω—ã–µ/–Ω–µ—á–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
        "X_train, X_test = X[:60000], X[60000:]\n",
        "y_train, y_test = y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ BERT –Ω–∞ IMDB\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(X_train.astype(str).tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(X_test.astype(str).tolist(), truncation=True, padding=True, max_length=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é\n",
        "- **–ó–∞–¥–∞—á–∞ 1:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ prompt –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è.\n",
        "- **–ó–∞–¥–∞—á–∞ 2:** –î–ª—è BERT –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `TFBertForSequenceClassification`.\n",
        "- **–ü–æ–¥—Å–∫–∞–∑–∫–∞:** –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è, —É–º–µ–Ω—å—à–∏—Ç–µ `num_train_epochs` –∏–ª–∏ —É–≤–µ–ª–∏—á—å—Ç–µ `weight_decay`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
